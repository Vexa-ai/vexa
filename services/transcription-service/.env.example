# Model configuration
MODEL_SIZE=large-v3-turbo

# Available models (all multilingual):
# - tiny, base, small, medium, large-v2, large-v3, large-v3-turbo
# 
# Recommended: large-v3-turbo + INT8
# - GPU VRAM: ~2.1 GB (validated)
# - Quality: Excellent (95-98% accuracy)
# - Speed: Very fast (>10x real-time)
# - Multilingual: 99+ languages

# Device configuration
# DEVICE=cuda  # For GPU (default)
# DEVICE=cpu   # For CPU-only

# Compute type (optimization)
# COMPUTE_TYPE=int8     # Default: 50-60% VRAM reduction, 2-4x CPU speedup, minimal accuracy loss
# COMPUTE_TYPE=float16  # GPU only: Maximum speed, higher VRAM usage (~6-8 GB)

# CPU optimization (only used when DEVICE=cpu)
# CPU_THREADS=4  # Set to number of physical CPU cores (0 = auto-detect)

# API Token for securing the service
# This token must match TRANSCRIPTION_SERVICE_API_TOKEN in the gateway
# If not set, service will accept all requests (not recommended for production)
# API_TOKEN=your_secure_token_here




