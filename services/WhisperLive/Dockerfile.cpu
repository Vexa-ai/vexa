FROM python:3.10-slim

# Set DEBIAN_FRONTEND to noninteractive to avoid prompts during apt-get install
ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    libsndfile1 \
    ffmpeg \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1
ENV OMP_NUM_THREADS=4

# Set working directory
WORKDIR /app

# Copy only the requirements file first
COPY services/WhisperLive/requirements/server.txt /tmp/requirements.txt

# Remove GPU-specific dependencies from requirements.txt
RUN sed -i '/openai-whisper/d' /tmp/requirements.txt || true \
    && sed -i '/onnxruntime==/d' /tmp/requirements.txt || true

# Install CPU-specific packages
RUN pip install --no-cache-dir -r /tmp/requirements.txt \
    && pip install --no-cache-dir onnxruntime \
    && pip install --no-cache-dir torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Install CPU-optimized faster-whisper
RUN pip install --no-cache-dir 'faster-whisper[cpu]'

# Create models directory and download the tiny model
RUN mkdir -p /app/models
RUN python -c "from faster_whisper import WhisperModel; model = WhisperModel('tiny', device='cpu', download_root='/app/models')"

# Now copy the application code
COPY services/WhisperLive/ /app/

# Copy our entrypoint script
COPY services/WhisperLive/entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# Set environment variables for the baked-in model
ENV WHISPER_MODEL_PATH=/app/models/faster-whisper-tiny
ENV WHISPER_MODEL_SIZE=tiny

# Set it as the entrypoint
ENTRYPOINT ["/entrypoint.sh"]

# Default command to run the server with faster_whisper backend
# Use the baked-in tiny model and more CPU threads for better performance
CMD ["--port", "9090", "--backend", "faster_whisper", "--model", "tiny", "--omp_num_threads", "4"] 