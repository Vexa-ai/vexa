# =============================================================================
# Vexa Monolithic - All-in-One Docker Image
# =============================================================================
#
# This Dockerfile creates a single container with all Vexa services:
# - API Gateway (port 8056)
# - Admin API (port 8057)
# - Bot Manager (with process orchestrator)
# - Transcription Collector
# - WhisperLive (transcription with CPU or GPU acceleration)
# - Vexa Bot (Node.js/Playwright)
#
# Designed for simple deployments on platforms like EasyPanel, Dokploy, etc.
# where Docker socket access is not available.
#
# =============================================================================
# BUILD EXAMPLES
# =============================================================================
#
# CPU (default - works everywhere):
#   docker build -f Dockerfile.monolithic -t vexa-monolithic .
#
# With embedded Redis (default) - only PostgreSQL needed:
#   docker run -e DATABASE_URL="postgresql://..." vexa-monolithic
#
# GPU with CUDA 12.4 (default GPU - recommended, driver 550+):
#   docker build -f Dockerfile.monolithic --build-arg DEVICE=gpu -t vexa-monolithic:gpu .
#
# GPU with CUDA 11.8 (maximum compatibility, driver 520+):
#   docker build -f Dockerfile.monolithic --build-arg DEVICE=gpu --build-arg CUDA_VERSION=11.8 -t vexa-monolithic:gpu-compat .
#
# GPU with CUDA 12.6 (recent hardware, driver 560+):
#   docker build -f Dockerfile.monolithic --build-arg DEVICE=gpu --build-arg CUDA_VERSION=12.6 -t vexa-monolithic:gpu-latest .
#
# GPU with CUDA 12.8 (Blackwell/RTX 5000 series, driver 570+):
#   docker build -f Dockerfile.monolithic --build-arg DEVICE=gpu --build-arg CUDA_VERSION=12.8 -t vexa-monolithic:gpu-blackwell .
#
# =============================================================================
# RUN EXAMPLES
# =============================================================================
#
# CPU (with embedded Redis - simplest):
#   docker run -d \
#     -p 8056:8056 -p 8057:8057 \
#     -e DATABASE_URL="postgresql://..." \
#     -e ADMIN_API_TOKEN="your-secret-token" \
#     vexa-monolithic
#
# CPU (with external Redis - optional):
#   docker run -d \
#     -p 8056:8056 -p 8057:8057 \
#     -e DATABASE_URL="postgresql://..." \
#     -e REDIS_URL="redis://external-redis:6379/0" \
#     -e ADMIN_API_TOKEN="your-secret-token" \
#     vexa-monolithic
#
# GPU (with embedded Redis):
#   docker run -d --gpus all \
#     -p 8056:8056 -p 8057:8057 \
#     -e DATABASE_URL="postgresql://..." \
#     -e ADMIN_API_TOKEN="your-secret-token" \
#     -e WHISPER_MODEL_SIZE=medium \
#     vexa-monolithic:gpu
#
# =============================================================================

# =============================================================================
# Build Arguments
# =============================================================================
# DEVICE: cpu (default) or gpu
# CUDA_VERSION: 11.8, 12.1, 12.4 (default), 12.6, 12.8 (Blackwell), or 12.9
ARG DEVICE=cpu
ARG CUDA_VERSION=12.4

# =============================================================================
# Stage 1: Base Images
# =============================================================================

# CPU base: Playwright with browsers pre-installed
FROM mcr.microsoft.com/playwright:v1.56.0-jammy AS base-cpu

# GPU bases for different CUDA versions (only the selected one is used)
# CUDA 11.8 - Maximum compatibility (driver 520+, supports Kepler to Ada)
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 AS base-gpu-11.8

# CUDA 12.1 - Good compatibility (driver 530+)
FROM nvidia/cuda:12.1.1-cudnn8-runtime-ubuntu22.04 AS base-gpu-12.1

# CUDA 12.4 - Recommended default (driver 550+, good balance)
FROM nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04 AS base-gpu-12.4

# CUDA 12.6 - Recent (driver 560+, latest features)
FROM nvidia/cuda:12.6.3-cudnn-runtime-ubuntu22.04 AS base-gpu-12.6

# CUDA 12.8 - Blackwell support (driver 570+, RTX 5000 series)
# This is the recommended version for RTX 5070/5080/5090 (sm_120)
FROM nvidia/cuda:12.8.1-cudnn-runtime-ubuntu22.04 AS base-gpu-12.8

# CUDA 12.9 - Latest (driver 570+, Blackwell architecture native support)
FROM nvidia/cuda:12.9.1-cudnn-runtime-ubuntu22.04 AS base-gpu-12.9

# =============================================================================
# Stage 2: Select GPU base based on CUDA_VERSION
# =============================================================================
FROM base-gpu-${CUDA_VERSION} AS base-gpu

# =============================================================================
# Stage 3: Final base selection (based on DEVICE arg)
# =============================================================================
FROM base-${DEVICE} AS base

# Re-declare ARGs after FROM (Docker requirement)
ARG DEVICE
ARG CUDA_VERSION

# Avoid prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Store build info for runtime detection
ENV BUILD_DEVICE=${DEVICE}
ENV BUILD_CUDA_VERSION=${CUDA_VERSION}

# -----------------------------------------------------------------------------
# System Dependencies (common)
# -----------------------------------------------------------------------------
RUN apt-get update && apt-get install -y --no-install-recommends \
    # Python 3.10+
    python3 \
    python3-pip \
    python3-venv \
    python3-dev \
    # Build tools (needed for some Python packages)
    build-essential \
    gcc \
    g++ \
    # Audio/Video processing
    xvfb \
    pulseaudio \
    ffmpeg \
    libsndfile1 \
    # Process management
    supervisor \
    # Database clients (for health checks and migrations)
    postgresql-client \
    # Redis server (embedded) and client tools
    redis-server \
    redis-tools \
    # Utilities
    curl \
    wget \
    git \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# -----------------------------------------------------------------------------
# GPU Only: Install Node.js (not in nvidia/cuda base image)
# Playwright browsers will be installed later from the bot directory
# -----------------------------------------------------------------------------
RUN if [ "$DEVICE" = "gpu" ]; then \
    echo "Installing Node.js for GPU build..." && \
    curl -fsSL https://deb.nodesource.com/setup_20.x | bash - && \
    apt-get install -y nodejs && \
    echo "Node.js $(node --version) installed"; \
    fi

# Create directory structure
WORKDIR /app
RUN mkdir -p \
    /app/api-gateway \
    /app/admin-api \
    /app/bot-manager \
    /app/transcription-collector \
    /app/whisperlive \
    /app/vexa-bot \
    /app/shared-models \
    /app/alembic \
    /var/log/supervisor \
    /var/log/vexa-bots \
    /data/redis

# -----------------------------------------------------------------------------
# Python Dependencies Stage
# -----------------------------------------------------------------------------

# Copy requirements files first
COPY docker/monolithic/requirements-monolithic.txt /tmp/requirements-monolithic.txt
COPY services/WhisperLive/requirements/server.txt /tmp/requirements-whisperlive.txt

# Install consolidated Python dependencies for all services
RUN pip3 install --no-cache-dir -r /tmp/requirements-monolithic.txt

# WhisperLive dependencies - CPU or GPU specific
# Note: openai-whisper is excluded (we use faster-whisper)
RUN sed -i '/openai-whisper/d' /tmp/requirements-whisperlive.txt || true \
    && sed -i '/onnxruntime==/d' /tmp/requirements-whisperlive.txt || true \
    && pip3 install --no-cache-dir -r /tmp/requirements-whisperlive.txt

# CPU-specific packages
RUN if [ "$DEVICE" = "cpu" ]; then \
    echo "Installing CPU-optimized packages..." && \
    pip3 install --no-cache-dir onnxruntime && \
    pip3 install --no-cache-dir torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cpu && \
    pip3 install --no-cache-dir 'faster-whisper' && \
    echo "CPU packages installed"; \
    fi

# GPU-specific packages with CUDA version mapping
# Maps CUDA_VERSION to appropriate PyTorch index:
#   11.8 -> cu118, 12.1 -> cu121, 12.4 -> cu124, 12.6/12.9 -> cu126
#   12.8 -> cu128 (Blackwell/RTX 5000 series support via PyTorch 2.7+)
# Note: Large downloads (~700MB for cuDNN), extended timeout for all pip operations
ENV PIP_DEFAULT_TIMEOUT=600
RUN if [ "$DEVICE" = "gpu" ]; then \
    echo "Installing GPU-accelerated packages for CUDA ${CUDA_VERSION}..." && \
    # Map CUDA version to PyTorch CUDA index and URL
    case "${CUDA_VERSION}" in \
        11.8) PYTORCH_CUDA="cu118"; PYTORCH_URL="https://download.pytorch.org/whl/cu118" ;; \
        12.1) PYTORCH_CUDA="cu121"; PYTORCH_URL="https://download.pytorch.org/whl/cu121" ;; \
        12.4) PYTORCH_CUDA="cu124"; PYTORCH_URL="https://download.pytorch.org/whl/cu124" ;; \
        12.6) PYTORCH_CUDA="cu126"; PYTORCH_URL="https://download.pytorch.org/whl/cu126" ;; \
        12.8) PYTORCH_CUDA="cu128"; PYTORCH_URL="https://download.pytorch.org/whl/cu128" ;; \
        12.9) PYTORCH_CUDA="cu126"; PYTORCH_URL="https://download.pytorch.org/whl/cu126" ;; \
        *) PYTORCH_CUDA="cu124"; PYTORCH_URL="https://download.pytorch.org/whl/cu124" ;; \
    esac && \
    echo "Using PyTorch index: ${PYTORCH_CUDA} from ${PYTORCH_URL}" && \
    pip3 install --no-cache-dir --retries 5 \
        torch torchvision torchaudio \
        --index-url ${PYTORCH_URL} && \
    pip3 install --no-cache-dir faster-whisper && \
    echo "GPU packages installed (CUDA ${CUDA_VERSION}, PyTorch ${PYTORCH_CUDA})"; \
    fi

# Clean up requirements files
RUN rm -f /tmp/requirements-*.txt

# GPU: Set NVIDIA library paths for cuBLAS/cuDNN
ENV LD_LIBRARY_PATH="${LD_LIBRARY_PATH:+$LD_LIBRARY_PATH:}/usr/local/lib/python3.10/site-packages/nvidia/cublas/lib:/usr/local/lib/python3.10/site-packages/nvidia/cudnn/lib"

# Copy shared library
COPY libs/shared-models /app/shared-models

# Add shared_models to Python path (more reliable than pip install)
ENV PYTHONPATH="/app/shared-models:${PYTHONPATH}"

# Verify shared_models is importable
RUN python3 -c "import shared_models; print(f'shared_models found at: {shared_models.__file__}')" \
    && python3 -c "from shared_models.models import User, Meeting; print('Models imported successfully')"

# -----------------------------------------------------------------------------
# Application Code
# -----------------------------------------------------------------------------

# API Gateway
COPY services/api-gateway/ /app/api-gateway/

# Admin API
COPY services/admin-api/ /app/admin-api/

# Bot Manager (including the new process orchestrator)
COPY services/bot-manager/app/ /app/bot-manager/app/

# Transcription Collector
COPY services/transcription-collector/ /app/transcription-collector/

# WhisperLive
COPY services/WhisperLive/ /app/whisperlive/

# Alembic migrations
COPY alembic.ini /app/alembic.ini
COPY libs/shared-models/alembic/ /app/alembic/

# -----------------------------------------------------------------------------
# Node.js Bot Build
# -----------------------------------------------------------------------------

# Copy vexa-bot source
COPY services/vexa-bot/core/ /app/vexa-bot/

# Build the bot
WORKDIR /app/vexa-bot
RUN npm ci --omit=dev 2>/dev/null || npm install \
    && npm run build

# Install Playwright browsers matching the bot's Playwright version
# This is critical: the bot's node_modules has its own Playwright version
# which may differ from globally installed Playwright
RUN npx playwright install chromium --with-deps \
    && npx playwright install msedge --with-deps || true

# Ensure browser-utils is built
RUN node build-browser-utils.js || true

# Return to app directory
WORKDIR /app

# -----------------------------------------------------------------------------
# Configuration Files
# -----------------------------------------------------------------------------

# Supervisor configuration
COPY docker/monolithic/supervisord.conf /etc/supervisor/conf.d/vexa.conf

# Entrypoint script
COPY docker/monolithic/entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

# -----------------------------------------------------------------------------
# Environment Variables (defaults)
# -----------------------------------------------------------------------------

# Orchestrator configuration
ENV ORCHESTRATOR=process \
    BOT_SCRIPT_PATH=/app/vexa-bot/dist/docker.js \
    BOT_WORKING_DIR=/app/vexa-bot \
    BOT_CALLBACK_BASE_URL=http://localhost:8080

# WhisperLive configuration
# DEVICE_TYPE will be auto-detected at runtime, but can be overridden
ENV WHISPER_LIVE_URL=ws://localhost:9090 \
    WHISPER_MODEL_SIZE=tiny \
    CONSUL_ENABLE=false

# Display for headless browsers
ENV DISPLAY=:99

# Logging
ENV LOG_LEVEL=info \
    PYTHONUNBUFFERED=1

# Database defaults (should be overridden)
# Note: DB_PASSWORD must be provided at runtime via -e flag
ENV DB_HOST=localhost \
    DB_PORT=5432 \
    DB_NAME=vexa \
    DB_USER=postgres

# Redis defaults (embedded by default, can use external via REDIS_URL)
ENV REDIS_HOST=localhost \
    REDIS_PORT=6379

# -----------------------------------------------------------------------------
# Ports
# -----------------------------------------------------------------------------

# API Gateway (main entry point)
EXPOSE 8056

# Admin API
EXPOSE 8057

# -----------------------------------------------------------------------------
# Health Check
# -----------------------------------------------------------------------------

HEALTHCHECK --interval=30s --timeout=10s --start-period=90s --retries=3 \
    CMD curl -sf http://localhost:8056/docs > /dev/null || exit 1

# -----------------------------------------------------------------------------
# Entrypoint
# -----------------------------------------------------------------------------

ENTRYPOINT ["/entrypoint.sh"]
CMD ["supervisord", "-n", "-c", "/etc/supervisor/conf.d/vexa.conf"]
